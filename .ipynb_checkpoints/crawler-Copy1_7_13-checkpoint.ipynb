{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawler\n",
    "A Web crawler, sometimes called a spider or spiderbot and often shortened to crawler, is an Internet bot that systematically browses the internet to Web indexing (web spidering). Here in my tests I am going to use **magazine luiza store** to get information and download some images. For help me on that, I am also using the library `BeautifulSoup` and `urllib`.\n",
    "\n",
    "[source: **keeping the same file format when saving .xlsx file using python**](https://stackoverflow.com/questions/51411809/keeping-the-same-file-format-when-saving-xlsx-file-using-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib\n",
    "import csv\n",
    "import re\n",
    "import xlsxwriter\n",
    "from openpyxl.workbook import Workbook\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is my web page that I want to crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get contents from url\n",
    "webpage = 'https://www.magazineluiza.com.br/smartphone-motorola-g7-play-32gb-indigo-4g-2gb-ram-tela-57-cam-13mp-cam-selfie-8mp/p/155549300/te/mtgp/'\n",
    "\n",
    "# get contents from url\n",
    "page = requests.get(webpage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are my functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(content):\n",
    "    # get soup\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    tag = soup.find('title', text=True)\n",
    "    if not tag:\n",
    "        return None\n",
    "    return tag.string.strip()\n",
    "\n",
    "def extractMax(input): \n",
    "    numbers = re.findall('\\d+',input)\n",
    "    numbers = map(int,numbers)\n",
    "    return max(numbers)\n",
    "\n",
    "def extract_old_price(content):\n",
    "    # get soup\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    tag = soup.select_one('.price-template__from')\n",
    "    if not tag:\n",
    "        return None\n",
    "    return extractMax(tag.string.strip())\n",
    "\n",
    "\n",
    "def extract_new_price(content):\n",
    "    # get soup\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    tag = soup.select_one('.price-template__text')\n",
    "    if not tag:\n",
    "        return None\n",
    "#     return tag.string.strip()\n",
    "    return extractMax(tag.string.strip())\n",
    "\n",
    "def extract_all_links(content):\n",
    "    # get soup\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    links = set()\n",
    "    for tag in soup.find_all('a', href=True):\n",
    "        if tag['href'].startswith('https://www.magazineluiza.com.br/'):\n",
    "            links.add(tag['href'])\n",
    "    return links\n",
    "\n",
    "def extract_img_links(content):\n",
    "    # get soup\n",
    "    soup = BeautifulSoup(content,'lxml')\n",
    "    img_links = set()    \n",
    "    for image_tag in soup.findAll(\"img\", {\"class\":\"carousel-product__item-img js-carousels-main-item-img\"}):\n",
    "        img_links.add(image_tag.get('src'))\n",
    "    return img_links\n",
    "\n",
    "def extract_showcase_link(content):\n",
    "    soup = BeautifulSoup(content,'lxml') # choose lxml parser\n",
    "    image_tags = soup.findAll('img', {\"class\":\"showcase-product__big-img\"})\n",
    "    for image_tag in image_tags:\n",
    "        return(image_tag.get('src'))\n",
    "    \n",
    "def download_showcase_img(content):\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    imgs = soup.findAll(\"img\", {\"class\":\"showcase-product__big-img\"})\n",
    "    for img in imgs:\n",
    "        img_url = urljoin(webpage, img['src'])\n",
    "        file_name = img['src'].split('/')[-1]\n",
    "        file_path = os.path.join(\"/Users/mattosoerick/Desktop/crawler/img/\", file_name)\n",
    "        urlretrieve(img_url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fazer o Craler de verdade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def crawl(start_url):\n",
    "#     seen_urls = set([start_url])\n",
    "#     available_urls = set([start_url])\n",
    "    \n",
    "#     header = [\n",
    "#         'titulo',\n",
    "#         'preco antigo',\n",
    "#         'preco novo',\n",
    "#         'link da vitrine',\n",
    "#         'link de todas as imagens do carrosel']\n",
    "    \n",
    "#     with open('myProducts.csv', 'w', encoding='UTF-8') as csvFile:\n",
    "#         writer = csv.writer(csvFile,delimiter=',')\n",
    "#         writer.writerow(header)   \n",
    "#     csvFile.close()\n",
    "    \n",
    "#     while available_urls:\n",
    "#         url = available_urls.pop()\n",
    "#         try:\n",
    "#             content = requests.get(url, timeout=3).text\n",
    "#         except Exception:\n",
    "#             continue\n",
    "\n",
    "#         for link in extract_all_links(content):\n",
    "#             if link not in seen_urls:\n",
    "#                 seen_urls.add(link)\n",
    "#                 available_urls.add(link)\n",
    "        \n",
    "#         if(extract_new_price(content)):\n",
    "#             print(extract_title(content))\n",
    "#             print(url)\n",
    "#             print(extract_old_price(content)) \n",
    "#             print(extract_new_price(content))\n",
    "#             print(extract_showcase_link(content))\n",
    "#             download_showcase_img(content)\n",
    "#             print()\n",
    "#             print()\n",
    "#             print()\n",
    "            \n",
    "#             lines = [\n",
    "#                 (extract_title(content)),\\\n",
    "#                 (url),\\\n",
    "#                 (extract_old_price(content)),\\\n",
    "#                 (extract_new_price(content))]\n",
    "            \n",
    "#             with open('myProducts.csv', 'a', encoding='UTF-8') as csvFile:\n",
    "\n",
    "#                 writer = csv.writer(csvFile,delimiter=',')\n",
    "#                 writer.writerow(lines)\n",
    "#             csvFile.close() \n",
    "\n",
    "# try:\n",
    "#     crawl(webpage)\n",
    "# except KeyboardInterrupt:\n",
    "#     print('Bye!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smartphone Motorola G7 Play 32GB Indigo 4G - 2GB RAM Tela 5,7” Câm. 13MP + Câm. Selfie 8MP - Moto G7 Play - Magazine Luiza\n",
      "https://www.magazineluiza.com.br/smartphone-motorola-g7-play-32gb-indigo-4g-2gb-ram-tela-57-cam-13mp-cam-selfie-8mp/p/155549300/te/mtgp/\n",
      "\n",
      "\n",
      "\n",
      "iPhone 8 Apple 64GB Dourado 4G Tela 4,7” - Retina Câm. 12MP + Selfie 7MP iOS 11 - iPhone - Magazine Luiza\n",
      "https://www.magazineluiza.com.br/iphone-8-apple-64gb-dourado-4g-tela-47-retina-cam-12mp-selfie-7mp-ios-11/p/155542800/te/teip/\n",
      "\n",
      "\n",
      "\n",
      "Cortador de Cabelo Mallory Mithos Power - 1 Velocidade - Máquina de Cortar Cabelo - Magazine Luiza\n",
      "https://www.magazineluiza.com.br/cortador-de-cabelo-mallory-mithos-power-1-velocidade/p/208569200/pf/macc/\n",
      "\n",
      "\n",
      "\n",
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "def crawl(start_url):\n",
    "    seen_urls = set([start_url])\n",
    "    available_urls = set([start_url])\n",
    "    \n",
    "    headers = [\n",
    "        'titulo',\n",
    "        'preco antigo',\n",
    "        'preco novo',\n",
    "        'link da vitrine',\n",
    "        'link de todas as imagens do carrosel']\n",
    "\n",
    "    workbook_name = 'uuuu.xlsx'\n",
    "    wb = Workbook()\n",
    "    page = wb.active\n",
    "    page.title = 'companies'\n",
    "    page.append(headers)\n",
    "    wb.save(filename = workbook_name)\n",
    "    \n",
    "    while available_urls:\n",
    "        url = available_urls.pop()\n",
    "        try:\n",
    "            content = requests.get(url, timeout=3).text\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        for link in extract_all_links(content):\n",
    "            if link not in seen_urls:\n",
    "                seen_urls.add(link)\n",
    "                available_urls.add(link)\n",
    "        \n",
    "        if(extract_new_price(content)):\n",
    "            print(extract_title(content))\n",
    "            print(url)\n",
    "#             print(extract_old_price(content)) \n",
    "#             print(extract_new_price(content))\n",
    "#             print(extract_showcase_link(content))\n",
    "#             download_showcase_img(content)\n",
    "            print()\n",
    "            print()\n",
    "            print()\n",
    "            \n",
    "            lines = [\n",
    "                (extract_title(content)),\\\n",
    "                (url),\\\n",
    "                (extract_old_price(content)),\\\n",
    "                (extract_new_price(content))]\n",
    "            \n",
    "                        \n",
    "            wb = load_workbook(workbook_name)\n",
    "            page = wb.active\n",
    "#             for info in lines:\n",
    "            page.append(lines)\n",
    "            wb.save(filename=workbook_name)\n",
    "            \n",
    "try:\n",
    "    crawl(webpage)\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print('Bye!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
