{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawler\n",
    "A Web crawler, sometimes called a spider or spiderbot and often shortened to crawler, is an Internet bot that systematically browses the internet to Web indexing (web spidering). Here in my tests I am going to use **magazine luiza store** to get information and download some images. For help me on that, I am also using the library `BeautifulSoup` and `urllib`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.parse import urlparse\n",
    "# from urllib.request import urlopen\n",
    "# import csv\n",
    "# import math\n",
    "# import pandas.io.formats.excel\n",
    "# import urllib\n",
    "# pandas.io.formats.excel.header_style = None\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.workbook import Workbook\n",
    "from urllib.parse import urljoin\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are my functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(content):\n",
    "    # get soup and choose lxml parser\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    # if title tag has text so return it\n",
    "    tag = soup.find('title', text=True)\n",
    "    # if title tag hasn't text so return none\n",
    "    if not tag:\n",
    "        return None\n",
    "    return tag.string.strip()\n",
    "\n",
    "def extractMax(value):\n",
    "    # replace characters\n",
    "    string = value.replace(\".\", \"\")\n",
    "    # replace characters\n",
    "    string = string.replace(\",\", \".\")\n",
    "    # remove string and keep just numbers\n",
    "    number = re.findall(r'-?\\d+\\.?\\d*',string)\n",
    "    return float(\"\".join(number))\n",
    "\n",
    "def extract_old_price(content):\n",
    "    # get soup and choose lxml parser\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    # if it finds a class, return its content\n",
    "    tag = soup.select_one('.price-template__from')\n",
    "    # if it does not find return none\n",
    "    if not tag:\n",
    "        return None\n",
    "    return extractMax(tag.string.strip())\n",
    "\n",
    "def extract_new_price(content):\n",
    "    # get soup and choose lxml parser\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    # if it finds a class, return its content\n",
    "    tag = soup.select_one('.price-template__text')\n",
    "    # if it does not find return none\n",
    "    if not tag:\n",
    "        return None\n",
    "    return extractMax(tag.string.strip())\n",
    "\n",
    "def extract_all_links(content):\n",
    "    # get soup and choose lxml parser\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    # set same as list, but it does not have duplicate values\n",
    "    links = set()\n",
    "    # find all links in a tag 'a' that starts with some rule and add to a \"list\" called links\n",
    "    for tag in soup.find_all('a', href=True):\n",
    "        if tag['href'].startswith('https://www.magazineluiza.com.br/'):\n",
    "            links.add(tag['href'])\n",
    "    return links\n",
    "\n",
    "def extract_showcase_link(content):\n",
    "    # get soup and choose lxml parser and choose lxml parser\n",
    "    soup = BeautifulSoup(content,'lxml') \n",
    "    # find the link in a tag 'img' inside some class and get its src\n",
    "    image_tags = soup.findAll('img', {\"class\":\"showcase-product__big-img\"})\n",
    "    for image_tag in image_tags:\n",
    "        return(image_tag.get('src'))\n",
    "    \n",
    "def download_showcase_img(content):\n",
    "    # get soup and choose lxml parser and choose lxml parser\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    # find the link in a tag 'img' inside some class\n",
    "    imgs = soup.findAll(\"img\", {\"class\":\"showcase-product__big-img\"})\n",
    "    # download img\n",
    "    for img in imgs:\n",
    "        # get the src of the img\n",
    "        img_url = urljoin(content, img['src'])\n",
    "        # it split all src link and get the final part to create the name of the file\n",
    "        file_name = img['src'].split('/')[-1]\n",
    "        # save this file on a folder called \"img/\"\n",
    "        file_path = os.path.join(\"img/\", file_name)\n",
    "        # actually downloads the img\n",
    "        urlretrieve(img_url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def crawl(content):\n",
    "    # get the first link and add to a list and, with set, ask to do not repeat the same url\n",
    "    seen_urls = set([content])\n",
    "    # get the first link and add to a list and, with set, ask to do not repeat the same url\n",
    "    available_urls = set([content])\n",
    "        \n",
    "    # create a workbook, there is no need to create a file on the filesystem to get started with openpyxl, \n",
    "    ## just import the workbook class and start work\n",
    "    wb = Workbook()\n",
    "    # a workbook is always created with at least one worksheet, you can get it by using the workbook.active \n",
    "    page = wb.active    \n",
    "    # name of the sheet\n",
    "    page.title = 'products'   \n",
    "    # header names list\n",
    "    headers = [\n",
    "        'product',\n",
    "        'old_price',\n",
    "        'new_price',\n",
    "        'discount',\n",
    "        'description',\n",
    "        'showcase_link',\n",
    "        'product_link']\n",
    "    # add header names list to our xlsx\n",
    "    page.append(headers)\n",
    "    # save xlsx file\n",
    "    workbook_name = 'magalu_raw.xlsx'\n",
    "    wb.save(filename = workbook_name)\n",
    "    # start a counter to present found product links\n",
    "    counter = 1\n",
    "    # select an existing excel file\n",
    "    wb = load_workbook(workbook_name)\n",
    "    # Crawler\n",
    "    while available_urls:\n",
    "        # all available url should be tested\n",
    "        url = available_urls.pop()\n",
    "        try:\n",
    "            # if it takes up to 3 seconds, so continue and add to a content variable\n",
    "            content = requests.get(url, timeout=3).text\n",
    "        except Exception:\n",
    "            continue\n",
    "            \n",
    "        print(str(counter) + \": \" + url)\n",
    "        counter+=1\n",
    "        \n",
    "        # for a link found, it should pass on extract_all_links\n",
    "        for link in extract_all_links(content):\n",
    "            # new link\n",
    "            if link not in seen_urls:\n",
    "                # add to a list of visited links\n",
    "                seen_urls.add(link)\n",
    "                # add to a list of available links to do the looping\n",
    "                available_urls.add(link)\n",
    "        \n",
    "        # if it finds a page with a price extract_new_price then it save data\n",
    "        if(extract_new_price(content)):\n",
    "            # if it finds a old price then it create the discount variable\n",
    "            if(extract_old_price(content)):\n",
    "                discount = (1-(extract_new_price(content)/extract_old_price(content)))\n",
    "            # if it does not finds a old price then it return None\n",
    "            else:\n",
    "                discount = None\n",
    "                \n",
    "            # pages with tag price download showcase img\n",
    "            download_showcase_img(content)\n",
    "\n",
    "            # select an existing excel sheet file\n",
    "            page = wb.active\n",
    "            # select data to save on an existing excel sheet file\n",
    "            lines = [\n",
    "                (extract_title(content)),\\\n",
    "                (extract_old_price(content)),\\\n",
    "                (extract_new_price(content)),\n",
    "                discount,\n",
    "                (extract_title(content)),\\\n",
    "                (extract_showcase_link(content)),\\\n",
    "                url]\n",
    "            # add on an existing excel sheet file\n",
    "            page.append(lines)\n",
    "            #save on an existing excel sheet file\n",
    "            wb.save(filename=workbook_name)\n",
    "            \n",
    "            # print the title of this page\n",
    "            print(\"It is a product: \" + extract_title(content)[0:51])\n",
    "            # start a counter to present found product links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get contents from url\n",
    "webpage = 'https://www.magazineluiza.com.br/smartphone-motorola-g7-play-32gb-indigo-4g-2gb-ram-tela-57-cam-13mp-cam-selfie-8mp/p/155549300/te/mtgp/'\n",
    "\n",
    "# get contents from url\n",
    "page = requests.get(webpage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: https://www.magazineluiza.com.br/smartphone-motorola-g7-play-32gb-indigo-4g-2gb-ram-tela-57-cam-13mp-cam-selfie-8mp/p/155549300/te/mtgp/\n",
      "It is a product: Smartphone Motorola G7 Play 32GB Indigo 4G - 2GB RA\n",
      "2: https://www.magazineluiza.com.br/suplementos-alimentares/l/sa/\n",
      "3: https://www.magazineluiza.com.br/galaxy-j6/celulares-e-smartphones/s/te/gaj6/\n",
      "4: https://www.magazineluiza.com.br/games/l/ga/\n",
      "5: https://www.magazineluiza.com.br/seu-espaco/\n",
      "6: https://www.magazineluiza.com.br/call-of-duty-black-ops-4-para-ps4-activision/p/221242600/ga/jcod/\n",
      "It is a product: Call of Duty Black Ops 4 para PS4 - Activision - Ca\n",
      "7: https://www.magazineluiza.com.br/pre-treino-t-rex-300g-laranja-vitafor-/p/5063975/sa/sa/\n",
      "It is a product: Pré Treino T-Rex 300g Laranja Vitafor - Suplementos\n",
      "8: https://www.magazineluiza.com.br/cama-mesa-e-banho/l/cm/\n",
      "9: https://www.magazineluiza.com.br/smartphone-samsung-galaxy-j6-32gb-violeta-dual-chip-4g-cam-13mp-selfie-8mp-flash-/p/cfce1244e8/te/gaj6/\n",
      "It is a product: Smartphone Samsung Galaxy J6 32GB Violeta - Dual Ch\n",
      "10: https://www.magazineluiza.com.br/esporte-e-lazer/l/es/\n",
      "11: https://www.magazineluiza.com.br/master-system-evolution-com-132-jogos-tectoy/p/205099200/ga/masy/\n",
      "\n",
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "# Start the crawler            \n",
    "try:\n",
    "    crawl(webpage)\n",
    "\n",
    "# Stop the while    \n",
    "except KeyboardInterrupt:\n",
    "    print()\n",
    "    print('Bye!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the excel file\n",
    "df = pd.read_excel (r'magalu_raw.xlsx')\n",
    "# organize 'discount' data in descending order\n",
    "df = df.sort_values(by=['discount'],ascending=False)\n",
    "# create a document called magalu_formated\n",
    "writer = pd.ExcelWriter('magalu_formated.xlsx', engine='xlsxwriter')\n",
    "# create a sheet called products\n",
    "df.to_excel(writer, sheet_name='products')\n",
    "# write in a sheet called products\n",
    "worksheet = writer.sheets['products']\n",
    "# format\n",
    "worksheet.conditional_format('E2:E999', {'type': '3_color_scale','min_color': \"green\",'max_color': \"red\"})\n",
    "# save it\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources\n",
    "[web crawler in python](https://medium.com/better-programming/develop-your-first-web-crawler-in-python-scrapy-6b2ee4baf954)<br>\n",
    "[how to crawl a web page with scrapy and python 3](https://www.digitalocean.com/community/tutorials/how-to-crawl-a-web-page-with-scrapy-and-python-3)<br>\n",
    "[Web Crawler to download all images from any website or webpage | Python 3.6](https://www.youtube.com/watch?v=gTJw7zKszfk)<br>\n",
    "[Markdown Format](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)<br>\n",
    "[Print Format](https://stackoverflow.com/questions/8450472/how-to-format-print-output-or-string-into-fixed-widthhttps://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)<br>\n",
    "[Conditional Colors for xlsx 1](https://github.com/jmcnamara/XlsxWriter/blob/master/dev/docs/source/working_with_conditional_formats.rst)<br>\n",
    "[Conditional Colors for xlsx 2](https://xlsxwriter.readthedocs.io/example_conditional_format.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
