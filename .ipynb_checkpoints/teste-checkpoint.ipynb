{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawler\n",
    "A Web crawler, sometimes called a spider or spiderbot and often shortened to crawler, is an Internet bot that systematically browses the internet to Web indexing (web spidering). Here in my tests I am going to use **magazine luiza store** to get information and download some images. For help me on that, I am also using the library `BeautifulSoup` and `urllib`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T00:37:15.870612Z",
     "start_time": "2019-07-20T00:37:14.135454Z"
    }
   },
   "outputs": [],
   "source": [
    "# from urllib.parse import urlparse\n",
    "# from urllib.request import urlopen\n",
    "# import csv\n",
    "# import math\n",
    "# import pandas.io.formats.excel\n",
    "# import urllib\n",
    "# pandas.io.formats.excel.header_style = None\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.workbook import Workbook\n",
    "from urllib.parse import urljoin\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are my functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T00:37:16.064502Z",
     "start_time": "2019-07-20T00:37:15.875832Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_title(content):\n",
    "    # get soup and choose lxml parser\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    # if title tag has text so return it\n",
    "    tag = soup.find('title', text=True)\n",
    "    # if title tag hasn't text so return none\n",
    "    if not tag:\n",
    "        return None\n",
    "    return tag.string.strip()\n",
    "\n",
    "def extractMax(value):\n",
    "    # replace characters\n",
    "    string = value.replace(\".\", \"\")\n",
    "    # replace characters\n",
    "    string = string.replace(\",\", \".\")\n",
    "    # remove string and keep just numbers\n",
    "    number = re.findall(r'-?\\d+\\.?\\d*',string)\n",
    "    return float(\"\".join(number))\n",
    "\n",
    "def extract_old_price(content):\n",
    "    # get soup and choose lxml parser\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    # if it finds a class, return its content\n",
    "    tag = soup.select_one('.price-template__from')\n",
    "    # if it does not find return none\n",
    "    if not tag:\n",
    "        return None\n",
    "    return extractMax(tag.string.strip())\n",
    "\n",
    "def extract_new_price(content):\n",
    "    # get soup and choose lxml parser\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    # if it finds a class, return its content\n",
    "    tag = soup.select_one('.price-template__text')\n",
    "    # if it does not find return none\n",
    "    if not tag:\n",
    "        return None\n",
    "    return extractMax(tag.string.strip())\n",
    "\n",
    "def extract_all_links(content):\n",
    "    # get soup and choose lxml parser\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    # set same as list, but it does not have duplicate values\n",
    "    links = set()\n",
    "    # find all links in a tag 'a' that starts with some rule and add to a \"list\" called links\n",
    "    for tag in soup.find_all('a', href=True):\n",
    "        if tag['href'].startswith('https://www.magazineluiza.com.br/'):\n",
    "            links.add(tag['href'])\n",
    "    return links\n",
    "\n",
    "def extract_showcase_link(content):\n",
    "    # get soup and choose lxml parser and choose lxml parser\n",
    "    soup = BeautifulSoup(content,'lxml') \n",
    "    # find the link in a tag 'img' inside some class and get its src\n",
    "    image_tags = soup.findAll('img', {\"class\":\"showcase-product__big-img\"})\n",
    "    for image_tag in image_tags:\n",
    "        return(image_tag.get('src'))\n",
    "    \n",
    "def download_showcase_img(content):\n",
    "    # get soup and choose lxml parser and choose lxml parser\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    # find the link in a tag 'img' inside some class\n",
    "    imgs = soup.findAll(\"img\", {\"class\":\"showcase-product__big-img\"})\n",
    "    # download img\n",
    "    for img in imgs:\n",
    "        # get the src of the img\n",
    "        img_url = urljoin(content, img['src'])\n",
    "        # it split all src link and get the final part to create the name of the file\n",
    "        file_name = img['src'].split('/')[-1]\n",
    "        # save this file on a folder called \"img/\"\n",
    "        file_path = os.path.join(\"img/\", file_name)\n",
    "        # actually downloads the img\n",
    "        urlretrieve(img_url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T00:37:16.225877Z",
     "start_time": "2019-07-20T00:37:16.067633Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def crawl(content):\n",
    "    # get the first link and add to a list and, with set, ask to do not repeat the same url\n",
    "    seen_urls = set([content])\n",
    "    # get the first link and add to a list and, with set, ask to do not repeat the same url\n",
    "    available_urls = set([content])\n",
    "        \n",
    "    # create a workbook, there is no need to create a file on the filesystem to get started with openpyxl, \n",
    "    ## just import the workbook class and start work\n",
    "    wb = Workbook()\n",
    "    # a workbook is always created with at least one worksheet, you can get it by using the workbook.active \n",
    "    page = wb.active    \n",
    "    # name of the sheet\n",
    "    page.title = 'products'   \n",
    "    # header names list\n",
    "    headers = [\n",
    "        'product',\n",
    "        'old_price',\n",
    "        'new_price',\n",
    "        'discount',\n",
    "        'description',\n",
    "        'showcase_link',\n",
    "        'product_link']\n",
    "    # add header names list to our xlsx\n",
    "    page.append(headers)\n",
    "    # save xlsx file\n",
    "    workbook_name = 'magalu_raw.xlsx'\n",
    "    wb.save(filename = workbook_name)\n",
    "    # start a counter to present found product links\n",
    "    counter = 1\n",
    "    # select an existing excel file\n",
    "    wb = load_workbook(workbook_name)\n",
    "    # Crawler\n",
    "    while available_urls:\n",
    "        # all available url should be tested\n",
    "        url = available_urls.pop()\n",
    "        try:\n",
    "            # if it takes up to 3 seconds, so continue and add to a content variable\n",
    "            content = requests.get(url, timeout=3).text\n",
    "        except Exception:\n",
    "            continue\n",
    "            \n",
    "        print(str(counter) + \": \" + url)\n",
    "        counter+=1\n",
    "        \n",
    "        # for a link found, it should pass on extract_all_links\n",
    "        for link in extract_all_links(content):\n",
    "            # new link\n",
    "            if link not in seen_urls:\n",
    "                # add to a list of visited links\n",
    "                seen_urls.add(link)\n",
    "                # add to a list of available links to do the looping\n",
    "                available_urls.add(link)\n",
    "        \n",
    "        # if it finds a page with a price extract_new_price then it save data\n",
    "        if(extract_new_price(content)):\n",
    "            # if it finds a old price then it create the discount variable\n",
    "            if(extract_old_price(content)):\n",
    "                discount = (1-(extract_new_price(content)/extract_old_price(content)))\n",
    "            # if it does not finds a old price then it return None\n",
    "            else:\n",
    "                discount = None\n",
    "                \n",
    "            # pages with tag price download showcase img\n",
    "            download_showcase_img(content)\n",
    "\n",
    "            # select an existing excel sheet file\n",
    "            page = wb.active\n",
    "            # select data to save on an existing excel sheet file\n",
    "            lines = [\n",
    "                (extract_title(content)),\\\n",
    "                (extract_old_price(content)),\\\n",
    "                (extract_new_price(content)),\n",
    "                discount,\n",
    "                (extract_title(content)),\\\n",
    "                (extract_showcase_link(content)),\\\n",
    "                url]\n",
    "            # add on an existing excel sheet file\n",
    "            page.append(lines)\n",
    "            #save on an existing excel sheet file\n",
    "            wb.save(filename=workbook_name)\n",
    "            \n",
    "            # print the title of this page\n",
    "            print(\"It is a product: \" + extract_title(content)[0:51])\n",
    "            # start a counter to present found product links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T00:37:16.932504Z",
     "start_time": "2019-07-20T00:37:16.228943Z"
    }
   },
   "outputs": [],
   "source": [
    "# get contents from url\n",
    "webpage = 'https://www.tudogostoso.com.br/receita/9031-caldo-verde.html'\n",
    "\n",
    "# get contents from url\n",
    "page = requests.get(webpage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T00:37:17.629466Z",
     "start_time": "2019-07-20T00:37:16.935344Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: https://www.tudogostoso.com.br/receita/9031-caldo-verde.html\n"
     ]
    }
   ],
   "source": [
    "# Start the crawler            \n",
    "try:\n",
    "    crawl(webpage)\n",
    "\n",
    "# Stop the while    \n",
    "except KeyboardInterrupt:\n",
    "    print()\n",
    "    print('Bye!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T00:37:17.902665Z",
     "start_time": "2019-07-20T00:37:17.636724Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the excel file\n",
    "df = pd.read_excel (r'magalu_raw.xlsx')\n",
    "# organize 'discount' data in descending order\n",
    "df = df.sort_values(by=['discount'],ascending=False)\n",
    "# create a document called magalu_formated\n",
    "writer = pd.ExcelWriter('magalu_formated.xlsx', engine='xlsxwriter')\n",
    "# create a sheet called products\n",
    "df.to_excel(writer, sheet_name='products')\n",
    "# write in a sheet called products\n",
    "worksheet = writer.sheets['products']\n",
    "# format\n",
    "worksheet.conditional_format('E2:E999', {'type': '3_color_scale','min_color': \"green\",'max_color': \"red\"})\n",
    "# save it\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources\n",
    "[web crawler in python](https://medium.com/better-programming/develop-your-first-web-crawler-in-python-scrapy-6b2ee4baf954)<br>\n",
    "[how to crawl a web page with scrapy and python 3](https://www.digitalocean.com/community/tutorials/how-to-crawl-a-web-page-with-scrapy-and-python-3)<br>\n",
    "[Web Crawler to download all images from any website or webpage | Python 3.6](https://www.youtube.com/watch?v=gTJw7zKszfk)<br>\n",
    "[Markdown Format](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)<br>\n",
    "[Print Format](https://stackoverflow.com/questions/8450472/how-to-format-print-output-or-string-into-fixed-widthhttps://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)<br>\n",
    "[Conditional Colors for xlsx 1](https://github.com/jmcnamara/XlsxWriter/blob/master/dev/docs/source/working_with_conditional_formats.rst)<br>\n",
    "[Conditional Colors for xlsx 2](https://xlsxwriter.readthedocs.io/example_conditional_format.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
