{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawler\n",
    "A Web crawler, sometimes called a spider or spiderbot and often shortened to crawler, is an Internet bot that systematically browses the internet to Web indexing (web spidering). Here in my tests I am going to use **magazine luiza store** to get information and download some images. For help me on that, I am also using the library `BeautifulSoup` and `urllib`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import pandas\n",
    "import requests\n",
    "import urllib\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is my web page that I want to crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get contents from url\n",
    "webpage = 'https://www.magazineluiza.com.br/smartphone-motorola-g7-play-32gb-indigo-4g-2gb-ram-tela-57-cam-13mp-cam-selfie-8mp/p/155549300/te/mtgp/'\n",
    "\n",
    "# get contents from url\n",
    "page = requests.get(webpage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are my functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(content):\n",
    "    # get soup\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    tag = soup.find('title', text=True)\n",
    "    if not tag:\n",
    "        return None\n",
    "    return tag.string.strip()\n",
    "\n",
    "def extract_old_price(content):\n",
    "    # get soup\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    tag = soup.select_one('.price-template__from')\n",
    "    if not tag:\n",
    "        return None\n",
    "#     return tag.string.strip()\n",
    "\n",
    "def extract_new_price(content):\n",
    "    # get soup\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    tag = soup.select_one('.price-template__text')\n",
    "    if not tag:\n",
    "        return None\n",
    "    return tag.string.strip()\n",
    "\n",
    "def extract_all_links(content):\n",
    "    # get soup\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    links = set()\n",
    "    for tag in soup.find_all('a', href=True):\n",
    "        if tag['href'].startswith('https://www.magazineluiza.com.br/'):\n",
    "            links.add(tag['href'])\n",
    "    return links\n",
    "\n",
    "def extract_img_links(content):\n",
    "    # get soup\n",
    "    soup = BeautifulSoup(content,'lxml')\n",
    "    img_links = set()    \n",
    "    for image_tag in soup.findAll(\"img\", {\"class\":\"carousel-product__item-img js-carousels-main-item-img\"}):\n",
    "        img_links.add(image_tag.get('src'))\n",
    "    return img_links\n",
    "\n",
    "def extract_showcase_link(content):\n",
    "    soup = BeautifulSoup(content,'lxml') # choose lxml parser\n",
    "    image_tags = soup.findAll('img', {\"class\":\"showcase-product__big-img\"})\n",
    "    for image_tag in image_tags:\n",
    "        return(image_tag.get('src'))\n",
    "    \n",
    "def download_showcase_img(content):\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    imgs = soup.findAll(\"img\", {\"class\":\"showcase-product__big-img\"})\n",
    "    for img in imgs:\n",
    "        img_url = urljoin(webpage, img['src'])\n",
    "        file_name = img['src'].split('/')[-1]\n",
    "        file_path = os.path.join(\"/Users/mattosoerick/Desktop/crawler/img/\", file_name)\n",
    "        urlretrieve(img_url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractMax(input): \n",
    "    numbers = re.findall('\\d+',input)\n",
    "    numbers = map(int,numbers)\n",
    "    return max(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fazer o Craler de verdade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smartphone Motorola G7 Play 32GB Indigo 4G - 2GB RAM Tela 5,7” Câm. 13MP + Câm. Selfie 8MP - Moto G7 Play - Magazine Luiza\n",
      "https://www.magazineluiza.com.br/smartphone-motorola-g7-play-32gb-indigo-4g-2gb-ram-tela-57-cam-13mp-cam-selfie-8mp/p/155549300/te/mtgp/\n",
      "de R$ 999,00\n",
      "836,07\n",
      "999\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-204e1dd5539f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwebpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Bye!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-204e1dd5539f>\u001b[0m in \u001b[0;36mcrawl\u001b[0;34m(start_url)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextractMax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_old_price\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mextract_new_price\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "def crawl(start_url):\n",
    "    seen_urls = set([start_url])\n",
    "    available_urls = set([start_url])\n",
    "    \n",
    "    header = [\n",
    "        'titulo',\n",
    "        'preco antigo',\n",
    "        'preco novo',\n",
    "        'link da vitrine',\n",
    "        'link de todas as imagens do carrosel']\n",
    "    with open('myProducts.csv', 'w') as csvFile:\n",
    "        writer = csv.writer(csvFile,delimiter=',')\n",
    "        writer.writerow(header)   \n",
    "    csvFile.close()\n",
    "    \n",
    "    while available_urls:\n",
    "        url = available_urls.pop()\n",
    "        try:\n",
    "            content = requests.get(url, timeout=3).text\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        for link in extract_all_links(content):\n",
    "            if link not in seen_urls:\n",
    "                seen_urls.add(link)\n",
    "                available_urls.add(link)\n",
    "        \n",
    "        if(extract_new_price(content)):\n",
    "            print(extract_title(content))\n",
    "            print(url)\n",
    "            print(extract_old_price(content))\n",
    "            print(extract_new_price(content))\n",
    "            print(extractMax(extract_old_price(content)))\n",
    "            \n",
    "            \n",
    "            print(extractMax(extract_old_price(content)) - extract_new_price(content))\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            print(extract_showcase_link(content))\n",
    "            download_showcase_img(content)\n",
    "            print()\n",
    "            print()\n",
    "            print()\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             lines = [\n",
    "#                 (extract_title(content)),\\\n",
    "#                 (url),\\\n",
    "#                 (extract_old_price(content)),\\\n",
    "#                 (extract_new_price(content))]\n",
    "            \n",
    "#             with open('myProducts.csv', 'a') as csvFile:\n",
    "#                 writer = csv.writer(csvFile,delimiter=',')\n",
    "#                 writer.writerow(lines)\n",
    "#             csvFile.close() \n",
    "\n",
    "\n",
    "try:\n",
    "    crawl(webpage)\n",
    "except KeyboardInterrupt:\n",
    "    print('Bye!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
